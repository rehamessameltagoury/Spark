{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submitted By: Reham ElTagoury\n",
    "# Group: 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vIwZsU0haNkj"
   },
   "source": [
    "## Task 1 - SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "45wCuSvgajao"
   },
   "source": [
    "### Build SparkSession:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JwcbnqiymCE3"
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/07 03:15:11 WARN Utils: Your hostname, ubuntu resolves to a loopback address: 127.0.1.1; using 192.168.130.131 instead (on interface ens33)\n",
      "21/10/07 03:15:11 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/opt/spark/jars/spark-unsafe_2.12-3.1.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "21/10/07 03:15:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/10/07 03:15:24 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "21/10/07 03:15:24 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "21/10/07 03:15:24 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('SparkSQL').enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g8HITwTqnJcX"
   },
   "source": [
    "### Read the json file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "93iqAB7tnMYQ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+---------+---+-----------+----+----------+-----+------+----+\n",
      "|   D|   H|      HDD| Id|      Model| RAM|ScreenSize|    W|Weight|Year|\n",
      "+----+----+---------+---+-----------+----+----------+-----+------+----+\n",
      "|9.48|0.61|512GB SSD|  1|MacBook Pro|16GB|       15\"|13.75|  4.02|2015|\n",
      "|7.74|0.52|256GB SSD|  2|    MacBook| 8GB|       12\"|11.04|  2.03|2016|\n",
      "|8.94|0.68|128GB SSD|  3|MacBook Air| 8GB|     13.3\"| 12.8|  2.96|2016|\n",
      "| 8.0|20.3|  1TB SSD|  4|       iMac|64GB|       27\"| 25.6|  20.8|2017|\n",
      "+----+----+---------+---+-----------+----+----------+-----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "JSON_PATH='DataFrames_sample.json'\n",
    "df=spark.read.json(JSON_PATH)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jNx0qMfunbKX"
   },
   "source": [
    "### Display the schema:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "UG4CcVJenc9y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- D: double (nullable = true)\n",
      " |-- H: double (nullable = true)\n",
      " |-- HDD: string (nullable = true)\n",
      " |-- Id: long (nullable = true)\n",
      " |-- Model: string (nullable = true)\n",
      " |-- RAM: string (nullable = true)\n",
      " |-- ScreenSize: string (nullable = true)\n",
      " |-- W: double (nullable = true)\n",
      " |-- Weight: double (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zaj0nHTcngEF"
   },
   "source": [
    "### Get all the data when \"Model\" equal \"MacBook Pro\":\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "Vm9QPKBCnkuS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+---------+---+-----------+----+----------+-----+------+----+\n",
      "|   D|   H|      HDD| Id|      Model| RAM|ScreenSize|    W|Weight|Year|\n",
      "+----+----+---------+---+-----------+----+----------+-----+------+----+\n",
      "|9.48|0.61|512GB SSD|  1|MacBook Pro|16GB|       15\"|13.75|  4.02|2015|\n",
      "+----+----+---------+---+-----------+----+----------+-----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df[df['Model']=='MacBook Pro'].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43oLte9LuGzA"
   },
   "source": [
    "### Create TempView:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "nVFYFcjtdIGW"
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"datatable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCLMmjRLdjbT"
   },
   "source": [
    "### Display \"RAM\"column and count \"RAM\" column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "BxykutRjuF0X"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:=================================================>      (66 + 2) / 75]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+\n",
      "| RAM|count(RAM)|\n",
      "+----+----------+\n",
      "|64GB|         1|\n",
      "|16GB|         1|\n",
      "| 8GB|         2|\n",
      "+----+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" select  RAM ,count(RAM) \n",
    "from datatable\n",
    "group by RAM\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5nlwq4t9gvK"
   },
   "source": [
    "### Get all columns when \"Year\" column equal \"2015\"  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "WXxjFxN19hJl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+---------+---+-----------+----+----------+-----+------+----+\n",
      "|   D|   H|      HDD| Id|      Model| RAM|ScreenSize|    W|Weight|Year|\n",
      "+----+----+---------+---+-----------+----+----------+-----+------+----+\n",
      "|9.48|0.61|512GB SSD|  1|MacBook Pro|16GB|       15\"|13.75|  4.02|2015|\n",
      "+----+----+---------+---+-----------+----+----------+-----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" select  * \n",
    "from datatable \n",
    "where Year =='2015'\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JHjK2Kqfuv24"
   },
   "source": [
    "### Get all when \"Model\" start with \"M\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "m30EkY_iu1Gs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+---------+---+-----------+----+----------+-----+------+----+\n",
      "|   D|   H|      HDD| Id|      Model| RAM|ScreenSize|    W|Weight|Year|\n",
      "+----+----+---------+---+-----------+----+----------+-----+------+----+\n",
      "|9.48|0.61|512GB SSD|  1|MacBook Pro|16GB|       15\"|13.75|  4.02|2015|\n",
      "|7.74|0.52|256GB SSD|  2|    MacBook| 8GB|       12\"|11.04|  2.03|2016|\n",
      "|8.94|0.68|128GB SSD|  3|MacBook Air| 8GB|     13.3\"| 12.8|  2.96|2016|\n",
      "+----+----+---------+---+-----------+----+----------+-----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" select  * \n",
    "from datatable \n",
    "where Model LIKE 'M%'\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Igw9iqJQ7TdH"
   },
   "source": [
    "### Get all data when \"Model\" column equal \"MacBook Pro\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "SRCGSB_W9cPc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+---------+---+-----------+----+----------+-----+------+----+\n",
      "|   D|   H|      HDD| Id|      Model| RAM|ScreenSize|    W|Weight|Year|\n",
      "+----+----+---------+---+-----------+----+----------+-----+------+----+\n",
      "|9.48|0.61|512GB SSD|  1|MacBook Pro|16GB|       15\"|13.75|  4.02|2015|\n",
      "+----+----+---------+---+-----------+----+----------+-----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" select  * \n",
    "from datatable \n",
    "where Model == 'MacBook Pro'\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OZIlmJidw1Ep"
   },
   "source": [
    "### Get all data with Multiple Conditions when \"RAM\" column equal \"8GB\" and \"Model\" column is \"Macbook\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "-5T7roxgnBBV"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+---------+---+-------+---+----------+-----+------+----+\n",
      "|   D|   H|      HDD| Id|  Model|RAM|ScreenSize|    W|Weight|Year|\n",
      "+----+----+---------+---+-------+---+----------+-----+------+----+\n",
      "|7.74|0.52|256GB SSD|  2|MacBook|8GB|       12\"|11.04|  2.03|2016|\n",
      "+----+----+---------+---+-------+---+----------+-----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" select  * \n",
    "from datatable \n",
    "where RAM == '8GB' and Model =='MacBook'\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qk8YPAWQ8HxI"
   },
   "source": [
    "### Get all data with Multiple Conditions when \"D\" greater than or equal \"8\" and \"Model\" column is \"iMac\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "XDHJSpQK9MuS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------+---+-----+----+----------+----+------+----+\n",
      "|  D|   H|    HDD| Id|Model| RAM|ScreenSize|   W|Weight|Year|\n",
      "+---+----+-------+---+-----+----+----------+----+------+----+\n",
      "|8.0|20.3|1TB SSD|  4| iMac|64GB|       27\"|25.6|  20.8|2017|\n",
      "+---+----+-------+---+-----+----+----------+----+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" select  * \n",
    "from datatable \n",
    "where D >= '8' and Model =='iMac'\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5d6364f6"
   },
   "source": [
    "## Task 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rlWhDvTPfZgu"
   },
   "source": [
    "### Read \"test1\" dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "7964d064"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_NAME='test1.csv'\n",
    "df1=(spark.read.format('csv')\n",
    "    .option('header','true')\n",
    "    .load(DATA_NAME)\n",
    ")\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.createOrReplaceTempView(\"test1_tbl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EJKjDOKHfnCt"
   },
   "source": [
    "### Display Salary of the people less than or equal to 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "c21edffc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+\n",
      "|   Name|age|Experience|Salary|\n",
      "+-------+---+----------+------+\n",
      "|  Sunny| 29|         4| 20000|\n",
      "|   Paul| 24|         3| 20000|\n",
      "| Harsha| 21|         1| 15000|\n",
      "|Shubham| 23|         2| 18000|\n",
      "+-------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "Select * from test1_tbl where Salary <=20000\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OvFWNFJjf0Pq"
   },
   "source": [
    "### Display Salary of the people less than or equal to 20000 and greater than or equal 15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "26f76ee1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+\n",
      "|   Name|age|Experience|Salary|\n",
      "+-------+---+----------+------+\n",
      "|  Sunny| 29|         4| 20000|\n",
      "|   Paul| 24|         3| 20000|\n",
      "| Harsha| 21|         1| 15000|\n",
      "|Shubham| 23|         2| 18000|\n",
      "+-------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "Select * from test1_tbl where Salary <=20000 and Salary >=15000\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VAcIXkTgN9D"
   },
   "source": [
    "## Task 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uOeqRO2KgW34"
   },
   "source": [
    "### Read \"test3\" dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "4d3bd081"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "DATA_NAME='test3.csv'\n",
    "df2=spark.read.csv(DATA_NAME, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejyUT1rngdeR"
   },
   "source": [
    "### Display dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "7ed791ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+\n",
      "|     Name| Departments|salary|\n",
      "+---------+------------+------+\n",
      "|    Krish|Data Science| 10000|\n",
      "|    Krish|         IOT|  5000|\n",
      "|   Mahesh|    Big Data|  4000|\n",
      "|    Krish|    Big Data|  4000|\n",
      "|   Mahesh|Data Science|  3000|\n",
      "|Sudhanshu|Data Science| 20000|\n",
      "|Sudhanshu|         IOT| 10000|\n",
      "|Sudhanshu|    Big Data|  5000|\n",
      "|    Sunny|Data Science| 10000|\n",
      "|    Sunny|    Big Data|  2000|\n",
      "+---------+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xp42YtorghXJ"
   },
   "source": [
    "### Display schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "d57d24ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Departments: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PHxWeGCCgnww"
   },
   "source": [
    "### Group by \"Name\" column and using sum function on \"Name\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "f15f8197"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 34:===========================================>            (58 + 2) / 75]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|     Name|Sum_Name|\n",
      "+---------+--------+\n",
      "|Sudhanshu|       3|\n",
      "|    Sunny|       2|\n",
      "|    Krish|       3|\n",
      "|   Mahesh|       2|\n",
      "+---------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df2.groupby('Name').agg({\"Name\":\"count\"}).withColumnRenamed(\"count(Name)\", \"Sum_Name\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gWgkaU3bhUOL"
   },
   "source": [
    "### Group by \"Name\" column and using avg function on \"Name\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 46:===================================================>    (69 + 2) / 75]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|     Name|Avg_Name|\n",
      "+---------+--------+\n",
      "|Sudhanshu|     0.3|\n",
      "|    Sunny|     0.2|\n",
      "|    Krish|     0.3|\n",
      "|   Mahesh|     0.2|\n",
      "+---------+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df2.createOrReplaceTempView(\"df_tbl\")\n",
    "df2_query=spark.sql(\"select Name,count(Name)/10 as Avg_Name from df_tbl group by Name \")\n",
    "df2_query.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S7rdLSEXhn4W"
   },
   "source": [
    "### Group by \"Departments\" column and using mean function on \"Departments\" column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "151d2264"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------------+\n",
      "| Departments|Departments_sum|\n",
      "+------------+---------------+\n",
      "|         IOT|              2|\n",
      "|    Big Data|              4|\n",
      "|Data Science|              4|\n",
      "+------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.groupby('Departments').agg({\"Departments\":\"count\"}).withColumnRenamed(\"count(Departments)\", \"Departments_sum\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------+\n",
      "| Departments|Departments_mean|\n",
      "+------------+----------------+\n",
      "|         IOT|             0.2|\n",
      "|    Big Data|             0.4|\n",
      "|Data Science|             0.4|\n",
      "+------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select Departments,count(Departments)/10 as Departments_mean from df_tbl group by Departments \").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bndivgGjhsbq"
   },
   "source": [
    "Group by \"Departments\" column and using count function on \"Departments\" column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "id": "bc7bf192"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------+\n",
      "| Departments|Departments_count|\n",
      "+------------+-----------------+\n",
      "|         IOT|                2|\n",
      "|    Big Data|                4|\n",
      "|Data Science|                4|\n",
      "+------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.groupby('Departments').agg({\"Departments\":\"count\"}).withColumnRenamed(\"count(Departments)\", \"Departments_count\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------------+\n",
      "| Departments|Departments_count|\n",
      "+------------+-----------------+\n",
      "|         IOT|                2|\n",
      "|    Big Data|                4|\n",
      "|Data Science|                4|\n",
      "+------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select Departments,count(Departments) as Departments_count from df_tbl group by Departments \").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dfPs99wnhwGu"
   },
   "source": [
    "### Apply agg to using sum function get the total of salaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "37b26cbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|salary_sum|\n",
      "+----------+\n",
      "|     73000|\n",
      "+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.select('salary').agg({\"salary\":\"sum\"}).withColumnRenamed(\"sum(salary)\", \"salary_sum\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MYD0wGPRi1FO"
   },
   "source": [
    "## Task 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJLc1PY1i-Np"
   },
   "source": [
    "You've been flown to their headquarters in Ulsan, South Korea, to assist them in accurately estimating the number of crew members a ship will need.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8PEoknoejL4r"
   },
   "source": [
    "They're currently building new ships for certain customers, and they'd like you to create a model and utilize it to estimate how many crew members the ships will require.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70slYH-tjR81"
   },
   "source": [
    "Metadata:\n",
    "1. Measurements of ship size \n",
    "2. capacity \n",
    "3. crew \n",
    "4. age for 158 cruise ships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5ZzrhGnHkRCU"
   },
   "source": [
    "It is saved in a csv file for you called \"ITI_data.csv\". our task is to develop a regression model that will assist in predicting the number of crew members required for future ships. The client also indicated that they have found that particular cruise lines will differ in acceptable crew counts, thus this is most likely an important factor to consider when conducting your investigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "A9CZzWWqZnOC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+---+------------------+----------+------+------+-----------------+----+\n",
      "|  Ship_name|Cruise_line|Age|           Tonnage|passengers|length|cabins|passenger_density|crew|\n",
      "+-----------+-----------+---+------------------+----------+------+------+-----------------+----+\n",
      "|    Journey|    Azamara|  6|30.276999999999997|      6.94|  5.94|  3.55|            42.64|3.55|\n",
      "|      Quest|    Azamara|  6|30.276999999999997|      6.94|  5.94|  3.55|            42.64|3.55|\n",
      "|Celebration|   Carnival| 26|            47.262|     14.86|  7.22|  7.43|             31.8| 6.7|\n",
      "|   Conquest|   Carnival| 11|             110.0|     29.74|  9.53| 14.88|            36.99|19.1|\n",
      "|    Destiny|   Carnival| 17|           101.353|     26.42|  8.92| 13.21|            38.36|10.0|\n",
      "|    Ecstasy|   Carnival| 22|            70.367|     20.52|  8.55|  10.2|            34.29| 9.2|\n",
      "|    Elation|   Carnival| 15|            70.367|     20.52|  8.55|  10.2|            34.29| 9.2|\n",
      "|    Fantasy|   Carnival| 23|            70.367|     20.56|  8.55| 10.22|            34.23| 9.2|\n",
      "|Fascination|   Carnival| 19|            70.367|     20.52|  8.55|  10.2|            34.29| 9.2|\n",
      "|    Freedom|   Carnival|  6|110.23899999999999|      37.0|  9.51| 14.87|            29.79|11.5|\n",
      "|      Glory|   Carnival| 10|             110.0|     29.74|  9.51| 14.87|            36.99|11.6|\n",
      "|    Holiday|   Carnival| 28|            46.052|     14.52|  7.27|  7.26|            31.72| 6.6|\n",
      "|Imagination|   Carnival| 18|            70.367|     20.52|  8.55|  10.2|            34.29| 9.2|\n",
      "|Inspiration|   Carnival| 17|            70.367|     20.52|  8.55|  10.2|            34.29| 9.2|\n",
      "|     Legend|   Carnival| 11|              86.0|     21.24|  9.63| 10.62|            40.49| 9.3|\n",
      "|   Liberty*|   Carnival|  8|             110.0|     29.74|  9.51| 14.87|            36.99|11.6|\n",
      "|    Miracle|   Carnival|  9|              88.5|     21.24|  9.63| 10.62|            41.67|10.3|\n",
      "|   Paradise|   Carnival| 15|            70.367|     20.52|  8.55|  10.2|            34.29| 9.2|\n",
      "|      Pride|   Carnival| 12|              88.5|     21.24|  9.63| 11.62|            41.67| 9.3|\n",
      "|  Sensation|   Carnival| 20|            70.367|     20.52|  8.55|  10.2|            34.29| 9.2|\n",
      "+-----------+-----------+---+------------------+----------+------+------+-----------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "DATA_NAME='ITI_data.csv'\n",
    "df3=spark.read.csv(DATA_NAME, header=True, inferSchema=True)\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------+---+-----------------+----------+------+------+-----------------+-----+\n",
      "|    Ship_name|     Cruise_line|Age|          Tonnage|passengers|length|cabins|passenger_density| crew|\n",
      "+-------------+----------------+---+-----------------+----------+------+------+-----------------+-----+\n",
      "|    Adventure| Royal_Caribbean| 12|            138.0|     31.14|  10.2| 15.57|            44.32|11.85|\n",
      "|      Allegra|           Costa| 21|            28.43|      8.08|  6.16|   4.1|            35.19|  4.0|\n",
      "|    Amsterdam|Holland_American| 13|             61.0|      13.8|   7.8|  6.88|             44.2|  6.0|\n",
      "|      Arcadia|             P&O|  9|             85.0|     19.68|  9.35|  9.84|            43.19| 8.69|\n",
      "|        Aries|            Star| 22|            3.341|      0.66|   2.8|  0.33|            50.62| 0.59|\n",
      "|      Armonia|             MSC| 12|             58.6|     15.66|  8.24|  7.83|            37.42|  7.0|\n",
      "|      Artemis|             P&O| 29|             45.0|     11.78|  7.54|   5.3|             38.2|  5.2|\n",
      "|    Atlantica|           Costa| 13|           85.619|     21.14|  9.57| 10.56|             40.5|  9.2|\n",
      "|       Aurora|             P&O| 13|             76.0|     18.74|  8.86|  9.39|            40.55|  8.5|\n",
      "|   Brilliance| Royal_Caribbean| 11|            90.09|     25.01|  9.62|  10.5|            36.02| 8.48|\n",
      "|    Caribbean|        Princess|  9|            116.0|      26.0|  9.51|  13.0|            44.62| 11.0|\n",
      "|  Celebration|        Carnival| 26|           47.262|     14.86|  7.22|  7.43|             31.8|  6.7|\n",
      "|      Century|       Celebrity| 18|70.60600000000001|      17.7|  8.15|  8.75|            39.89| 8.58|\n",
      "|     Classica|           Costa| 22|           52.926|     13.02|  7.18|  6.54|            40.65| 6.17|\n",
      "|        Cloud|       Silversea| 19|             16.8|      2.96|  5.14|  1.48|            56.76|  2.1|\n",
      "|     Conquest|        Carnival| 11|            110.0|     29.74|  9.53| 14.88|            36.99| 19.1|\n",
      "|Constellation|       Celebrity| 11|             91.0|     20.32|  9.65|  9.75|            44.78| 9.99|\n",
      "|        Coral|        Princess| 11|91.62700000000001|     19.74|  9.64|  9.87|            46.42|  9.0|\n",
      "|        Crown|       Norwegian| 25|            34.25|     10.52|  6.15|  5.26|            32.56|  4.7|\n",
      "|        Crown|        Princess|  7|            116.0|      31.0|  9.51| 15.57|            37.42| 12.0|\n",
      "+-------------+----------------+---+-----------------+----------+------+------+-----------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train,test=df3.randomSplit([.8,.2],seed=0)\n",
    "train.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1QTNLhZSlf9_"
   },
   "source": [
    "### OneHotEncoder \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "id": "-ZZxxxKLZnOF"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ship_name', 'string'),\n",
       " ('Cruise_line', 'string'),\n",
       " ('Age', 'int'),\n",
       " ('Tonnage', 'double'),\n",
       " ('passengers', 'double'),\n",
       " ('length', 'double'),\n",
       " ('cabins', 'double'),\n",
       " ('passenger_density', 'double'),\n",
       " ('crew', 'double')]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import numpy\n",
    "train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cruise_line']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categoricalCols = [field for (field, dataType) in train.dtypes\n",
    "                   if dataType == \"string\"]\n",
    "categoricalCols.remove('Ship_name')\n",
    "categoricalCols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cruise_line_Index']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexOutputCols = [x + \"_Index\" for x in categoricalCols]\n",
    "indexOutputCols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Cruise_line_OHE']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oheOutputCols = [x + \"_OHE\" for x in categoricalCols]\n",
    "oheOutputCols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "stringIndexer = StringIndexer(inputCols=categoricalCols,\n",
    "                             outputCols=indexOutputCols,\n",
    "                             handleInvalid='skip')\n",
    "oheEncoder = OneHotEncoder(inputCols=indexOutputCols,\n",
    "                          outputCols=oheOutputCols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Age', 'Tonnage', 'passengers', 'length', 'cabins', 'passenger_density']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numericCols = [field for (field,dataType) in train.dtypes\n",
    "              if (((dataType=='double')or(dataType=='int')) & (field!='crew'))]\n",
    "numericCols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RNCxWem0l662"
   },
   "source": [
    "###Use VectorAssembler to merge all columns into one column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "pE4ohNjVZnOG"
   },
   "outputs": [],
   "source": [
    "vecAssembler = VectorAssembler(inputCols=oheOutputCols+numericCols,outputCol='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rbf56f6AmUUl"
   },
   "source": [
    "### Create a Linear Regression Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "nvqnqTkunkNx"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "lr = LinearRegression(labelCol='crew',featuresCol='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVdxQTcSC6Cz"
   },
   "source": [
    "### Creating a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "UqM9HxkNIHwE"
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[stringIndexer,oheEncoder,vecAssembler,lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/10/07 03:30:51 WARN Instrumentation: [bc2106cd] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    }
   ],
   "source": [
    "pipelineModel = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "predDF = pipelineModel.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DEbfwhqeHOCc"
   },
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE is 0.7\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "regressionEvaluator = RegressionEvaluator(predictionCol='prediction',\n",
    "                                         labelCol='crew',\n",
    "                                         metricName='rmse')\n",
    "rmse = regressionEvaluator.evaluate(predDF)\n",
    "\n",
    "print(f\"RMSE is {rmse:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 is 0.9527682330125713\n"
     ]
    }
   ],
   "source": [
    "# Using R^2\n",
    "r2 = RegressionEvaluator(predictionCol='prediction',\n",
    "                                         labelCol='crew',\n",
    "                                         metricName='r2').evaluate(predDF)\n",
    "print(f\"R2 is {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SluXM6j-Gt9B"
   },
   "source": [
    "By Eng. Mostafa Nabieh \n",
    "If you have questions, please feel free to ask.\n",
    "\n",
    "My Email : nabieh.mostafa@yahoo.com\n",
    "\n",
    "My Whatsapp : +201015197566"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Session 2 H.W.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
